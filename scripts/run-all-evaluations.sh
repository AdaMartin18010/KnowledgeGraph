#!/bin/bash

# çŸ¥è¯†å›¾è°±é¡¹ç›®ç»¼åˆè¯„æµ‹è„šæœ¬ / Knowledge Graph Project Comprehensive Evaluation Script
# ä½œè€… / Author: KnowledgeGraph Team
# ç‰ˆæœ¬ / Version: 1.0.0

set -e

# é¢œè‰²å®šä¹‰ / Color Definitions
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•° / Logging Functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_step() {
    echo -e "${CYAN}[STEP]${NC} $1"
}

# æ£€æŸ¥ç¯å¢ƒ / Check Environment
check_environment() {
    log_info "æ£€æŸ¥ç»¼åˆè¯„æµ‹ç¯å¢ƒ / Checking comprehensive evaluation environment..."
    
    # æ£€æŸ¥Pythonç¯å¢ƒ / Check Python environment
    if ! command -v python &> /dev/null; then
        log_error "Pythonæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
        exit 1
    fi
    
    # æ£€æŸ¥Dockerç¯å¢ƒ / Check Docker environment
    if ! command -v docker &> /dev/null; then
        log_warning "Dockeræœªå®‰è£…ï¼Œå°†è·³è¿‡å®¹å™¨ç¯å¢ƒè¯„æµ‹"
        DOCKER_AVAILABLE=false
    else
        if docker info &> /dev/null; then
            log_success "Dockerç¯å¢ƒå¯ç”¨ / Docker environment available"
            DOCKER_AVAILABLE=true
        else
            log_warning "Dockeræœªè¿è¡Œï¼Œå°†è·³è¿‡å®¹å™¨ç¯å¢ƒè¯„æµ‹"
            DOCKER_AVAILABLE=false
        fi
    fi
    
    # åˆ›å»ºç»“æœç›®å½• / Create results directory
    mkdir -p results
    log_success "ç¯å¢ƒæ£€æŸ¥å®Œæˆ / Environment check completed"
}

# è¿è¡ŒçŸ¥è¯†è¡¨ç¤ºè¯„æµ‹ / Run Knowledge Representation Evaluation
run_kr_evaluation() {
    log_step "è¿è¡ŒçŸ¥è¯†è¡¨ç¤ºæ¨¡å—è¯„æµ‹ / Running Knowledge Representation module evaluation"
    
    if [ -f "scripts/kr_eval.sh" ]; then
        bash scripts/kr_eval.sh
        log_success "çŸ¥è¯†è¡¨ç¤ºè¯„æµ‹å®Œæˆ / Knowledge Representation evaluation completed"
    else
        log_warning "çŸ¥è¯†è¡¨ç¤ºè¯„æµ‹è„šæœ¬æœªæ‰¾åˆ°ï¼Œè·³è¿‡ / KR evaluation script not found, skipping"
    fi
}

# è¿è¡Œå›¾è®ºè¯„æµ‹ / Run Graph Theory Evaluation
run_gt_evaluation() {
    log_step "è¿è¡Œå›¾è®ºæ¨¡å—è¯„æµ‹ / Running Graph Theory module evaluation"
    
    if [ -f "scripts/gt_eval.sh" ]; then
        bash scripts/gt_eval.sh
        log_success "å›¾è®ºè¯„æµ‹å®Œæˆ / Graph Theory evaluation completed"
    else
        log_warning "å›¾è®ºè¯„æµ‹è„šæœ¬æœªæ‰¾åˆ°ï¼Œè·³è¿‡ / GT evaluation script not found, skipping"
    fi
}

# è¿è¡Œè¯­ä¹‰åˆ†æè¯„æµ‹ / Run Semantic Analysis Evaluation
run_sa_evaluation() {
    log_step "è¿è¡Œè¯­ä¹‰åˆ†ææ¨¡å—è¯„æµ‹ / Running Semantic Analysis module evaluation"
    
    # åˆ›å»ºè¯­ä¹‰åˆ†æè¯„æµ‹è„šæœ¬ / Create semantic analysis evaluation script
    mkdir -p results/semantic-analysis
    
    python << 'EOF'
import spacy
import nltk
import transformers
import time
import json
from datetime import datetime

print("=== è¯­ä¹‰åˆ†ææ¨¡å—è¯„æµ‹ / Semantic Analysis Module Evaluation ===")
print(f"è¯„æµ‹æ—¶é—´ / Evaluation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print()

# 1. ç¯å¢ƒä¿¡æ¯ / Environment Information
print("1. ç¯å¢ƒä¿¡æ¯ / Environment Information")
try:
    print(f"   spaCyç‰ˆæœ¬ / spaCy Version: {spacy.__version__}")
    print(f"   NLTKç‰ˆæœ¬ / NLTK Version: {nltk.__version__}")
    print(f"   Transformersç‰ˆæœ¬ / Transformers Version: {transformers.__version__}")
except Exception as e:
    print(f"   ç‰ˆæœ¬ä¿¡æ¯è·å–å¤±è´¥ / Version info failed: {e}")
print()

# 2. spaCyæ¨¡å‹æµ‹è¯• / spaCy Model Test
print("2. spaCyæ¨¡å‹æµ‹è¯• / spaCy Model Test")
start_time = time.time()
try:
    nlp = spacy.load("en_core_web_sm")
    load_time = time.time() - start_time
    print(f"   âœ… spaCyæ¨¡å‹åŠ è½½æˆåŠŸ / spaCy model loaded successfully")
    print(f"   åŠ è½½æ—¶é—´ / Loading time: {load_time:.2f}s")
    
    # æ–‡æœ¬å¤„ç†æµ‹è¯• / Text processing test
    text = "Knowledge Graph is an important technology for artificial intelligence."
    doc = nlp(text)
    print(f"   æ–‡æœ¬å¤„ç†æˆåŠŸ / Text processing successful")
    print(f"   å®ä½“æ•°é‡ / Entity count: {len(doc.ents)}")
    
except Exception as e:
    print(f"   âŒ spaCyæ¨¡å‹æµ‹è¯•å¤±è´¥ / spaCy model test failed: {e}")
print()

# 3. NLTKæµ‹è¯• / NLTK Test
print("3. NLTKæµ‹è¯• / NLTK Test")
try:
    from nltk.tokenize import word_tokenize
    from nltk.tag import pos_tag
    
    text = "Knowledge Graph technology enables semantic understanding."
    tokens = word_tokenize(text)
    pos_tags = pos_tag(tokens)
    
    print(f"   âœ… NLTKæµ‹è¯•æˆåŠŸ / NLTK test successful")
    print(f"   åˆ†è¯ç»“æœ / Tokenization: {tokens}")
    print(f"   è¯æ€§æ ‡æ³¨ / POS tagging: {pos_tags}")
    
except Exception as e:
    print(f"   âŒ NLTKæµ‹è¯•å¤±è´¥ / NLTK test failed: {e}")
print()

# 4. Transformersæµ‹è¯• / Transformers Test
print("4. Transformersæµ‹è¯• / Transformers Test")
start_time = time.time()
try:
    from transformers import pipeline
    
    classifier = pipeline("sentiment-analysis")
    result = classifier("Knowledge Graph is a great technology!")
    
    tf_time = time.time() - start_time
    print(f"   âœ… Transformersæµ‹è¯•æˆåŠŸ / Transformers test successful")
    print(f"   æƒ…æ„Ÿåˆ†æç»“æœ / Sentiment analysis: {result}")
    print(f"   å¤„ç†æ—¶é—´ / Processing time: {tf_time:.2f}s")
    
except Exception as e:
    print(f"   âŒ Transformersæµ‹è¯•å¤±è´¥ / Transformers test failed: {e}")
print()

# ä¿å­˜ç»“æœ / Save results
results = {
    "evaluation_time": datetime.now().isoformat(),
    "tests": {
        "spacy_test": "PASS" if 'nlp' in locals() else "FAIL",
        "nltk_test": "PASS" if 'tokens' in locals() else "FAIL",
        "transformers_test": "PASS" if 'result' in locals() else "FAIL"
    }
}

with open("results/semantic-analysis/evaluation_results.json", "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print()
print("=== è¯„æµ‹å®Œæˆ / Evaluation Completed ===")
print(f"ç»“æœå·²ä¿å­˜åˆ°: results/semantic-analysis/evaluation_results.json")
EOF

    log_success "è¯­ä¹‰åˆ†æè¯„æµ‹å®Œæˆ / Semantic Analysis evaluation completed"
}

# è¿è¡Œæœ¬ä½“å·¥ç¨‹è¯„æµ‹ / Run Ontology Engineering Evaluation
run_oe_evaluation() {
    log_step "è¿è¡Œæœ¬ä½“å·¥ç¨‹æ¨¡å—è¯„æµ‹ / Running Ontology Engineering module evaluation"
    
    mkdir -p results/ontology-engineering
    
    python << 'EOF'
import rdflib
import owlready2
import time
import json
from datetime import datetime

print("=== æœ¬ä½“å·¥ç¨‹æ¨¡å—è¯„æµ‹ / Ontology Engineering Module Evaluation ===")
print(f"è¯„æµ‹æ—¶é—´ / Evaluation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print()

# 1. RDFå¤„ç†æµ‹è¯• / RDF Processing Test
print("1. RDFå¤„ç†æµ‹è¯• / RDF Processing Test")
start_time = time.time()
try:
    g = rdflib.Graph()
    g.add((rdflib.URIRef("http://example.org/Person"), 
            rdflib.RDF.type, 
            rdflib.OWL.Class))
    g.add((rdflib.URIRef("http://example.org/Person"), 
            rdflib.RDFS.label, 
            rdflib.Literal("Person")))
    
    rdf_time = time.time() - start_time
    print(f"   âœ… RDFå¤„ç†æµ‹è¯•æˆåŠŸ / RDF processing test successful")
    print(f"   ä¸‰å…ƒç»„æ•°é‡ / Triple count: {len(g)}")
    print(f"   å¤„ç†æ—¶é—´ / Processing time: {rdf_time:.4f}s")
    
except Exception as e:
    print(f"   âŒ RDFå¤„ç†æµ‹è¯•å¤±è´¥ / RDF processing test failed: {e}")
print()

# 2. OWLå¤„ç†æµ‹è¯• / OWL Processing Test
print("2. OWLå¤„ç†æµ‹è¯• / OWL Processing Test")
start_time = time.time()
try:
    from owlready2 import *
    
    onto = get_ontology("http://test.org/onto.owl")
    with onto:
        class Person(Thing):
            pass
        
        class hasName(DataProperty):
            domain = [Person]
            range = [str]
    
    owl_time = time.time() - start_time
    print(f"   âœ… OWLå¤„ç†æµ‹è¯•æˆåŠŸ / OWL processing test successful")
    print(f"   ç±»æ•°é‡ / Class count: {len(onto.classes())}")
    print(f"   å±æ€§æ•°é‡ / Property count: {len(onto.data_properties())}")
    print(f"   å¤„ç†æ—¶é—´ / Processing time: {owl_time:.4f}s")
    
except Exception as e:
    print(f"   âŒ OWLå¤„ç†æµ‹è¯•å¤±è´¥ / OWL processing test failed: {e}")
print()

# ä¿å­˜ç»“æœ / Save results
results = {
    "evaluation_time": datetime.now().isoformat(),
    "tests": {
        "rdf_test": "PASS" if 'g' in locals() else "FAIL",
        "owl_test": "PASS" if 'onto' in locals() else "FAIL"
    }
}

with open("results/ontology-engineering/evaluation_results.json", "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print()
print("=== è¯„æµ‹å®Œæˆ / Evaluation Completed ===")
print(f"ç»“æœå·²ä¿å­˜åˆ°: results/ontology-engineering/evaluation_results.json")
EOF

    log_success "æœ¬ä½“å·¥ç¨‹è¯„æµ‹å®Œæˆ / Ontology Engineering evaluation completed"
}

# ç”Ÿæˆç»¼åˆæŠ¥å‘Š / Generate Comprehensive Report
generate_comprehensive_report() {
    log_step "ç”Ÿæˆç»¼åˆè¯„æµ‹æŠ¥å‘Š / Generating comprehensive evaluation report"
    
    python << 'EOF'
import json
import os
import glob
from datetime import datetime

print("=== ç”Ÿæˆç»¼åˆè¯„æµ‹æŠ¥å‘Š / Generating Comprehensive Evaluation Report ===")

# æ”¶é›†æ‰€æœ‰ç»“æœ / Collect all results
results_files = glob.glob("results/*/evaluation_results.json")
comprehensive_results = {
    "report_time": datetime.now().isoformat(),
    "total_modules": len(results_files),
    "modules": {},
    "summary": {
        "total_tests": 0,
        "passed_tests": 0,
        "failed_tests": 0
    }
}

for result_file in results_files:
    module_name = result_file.split('/')[1]
    try:
        with open(result_file, 'r', encoding='utf-8') as f:
            module_results = json.load(f)
        
        comprehensive_results["modules"][module_name] = module_results
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ / Count test results
        if "tests" in module_results:
            tests = module_results["tests"]
            comprehensive_results["summary"]["total_tests"] += len(tests)
            comprehensive_results["summary"]["passed_tests"] += sum(1 for test in tests.values() if test == "PASS")
            comprehensive_results["summary"]["failed_tests"] += sum(1 for test in tests.values() if test == "FAIL")
    
    except Exception as e:
        print(f"è¯»å– {result_file} å¤±è´¥: {e}")

# è®¡ç®—æˆåŠŸç‡ / Calculate success rate
if comprehensive_results["summary"]["total_tests"] > 0:
    success_rate = (comprehensive_results["summary"]["passed_tests"] / comprehensive_results["summary"]["total_tests"]) * 100
    comprehensive_results["summary"]["success_rate"] = f"{success_rate:.1f}%"
else:
    comprehensive_results["summary"]["success_rate"] = "0%"

# ä¿å­˜ç»¼åˆæŠ¥å‘Š / Save comprehensive report
with open("results/comprehensive_evaluation_report.json", "w", encoding="utf-8") as f:
    json.dump(comprehensive_results, f, ensure_ascii=False, indent=2)

# æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦ / Display report summary
print(f"\nğŸ“Š ç»¼åˆè¯„æµ‹æŠ¥å‘Šæ‘˜è¦ / Comprehensive Evaluation Report Summary")
print(f"   è¯„æµ‹æ—¶é—´ / Evaluation Time: {comprehensive_results['report_time']}")
print(f"   è¯„æµ‹æ¨¡å—æ•° / Total Modules: {comprehensive_results['total_modules']}")
print(f"   æ€»æµ‹è¯•æ•° / Total Tests: {comprehensive_results['summary']['total_tests']}")
print(f"   é€šè¿‡æµ‹è¯•æ•° / Passed Tests: {comprehensive_results['summary']['passed_tests']}")
print(f"   å¤±è´¥æµ‹è¯•æ•° / Failed Tests: {comprehensive_results['summary']['failed_tests']}")
print(f"   æˆåŠŸç‡ / Success Rate: {comprehensive_results['summary']['success_rate']}")

print(f"\nâœ… ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: results/comprehensive_evaluation_report.json")
EOF

    log_success "ç»¼åˆè¯„æµ‹æŠ¥å‘Šç”Ÿæˆå®Œæˆ / Comprehensive evaluation report generated"
}

# æ˜¾ç¤ºæœ€ç»ˆç»“æœ / Show Final Results
show_final_results() {
    log_info "ç»¼åˆè¯„æµ‹ç»“æœ / Comprehensive Evaluation Results:"
    
    if [ -f "results/comprehensive_evaluation_report.json" ]; then
        echo ""
        echo "ğŸ“Š ç»¼åˆæŠ¥å‘Šæ‘˜è¦ / Comprehensive Report Summary:"
        cat results/comprehensive_evaluation_report.json | python -c "
import json, sys
data = json.load(sys.stdin)
print(f'è¯„æµ‹æ—¶é—´: {data[\"report_time\"]}')
print(f'è¯„æµ‹æ¨¡å—æ•°: {data[\"total_modules\"]}')
print(f'æ€»æµ‹è¯•æ•°: {data[\"summary\"][\"total_tests\"]}')
print(f'é€šè¿‡æµ‹è¯•æ•°: {data[\"summary\"][\"passed_tests\"]}')
print(f'å¤±è´¥æµ‹è¯•æ•°: {data[\"summary\"][\"failed_tests\"]}')
print(f'æˆåŠŸç‡: {data[\"summary\"][\"success_rate\"]}')
"
    else
        log_warning "æœªæ‰¾åˆ°ç»¼åˆæŠ¥å‘Šæ–‡ä»¶ / Comprehensive report file not found"
    fi
}

# ä¸»å‡½æ•° / Main function
main() {
    log_info "å¼€å§‹çŸ¥è¯†å›¾è°±é¡¹ç›®ç»¼åˆè¯„æµ‹ / Starting Knowledge Graph Project comprehensive evaluation"
    echo ""
    
    # æ£€æŸ¥ç¯å¢ƒ / Check environment
    check_environment
    
    # è¿è¡Œå„æ¨¡å—è¯„æµ‹ / Run module evaluations
    run_kr_evaluation
    echo ""
    
    run_gt_evaluation
    echo ""
    
    run_sa_evaluation
    echo ""
    
    run_oe_evaluation
    echo ""
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š / Generate comprehensive report
    generate_comprehensive_report
    echo ""
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ / Show final results
    show_final_results
    
    log_success "çŸ¥è¯†å›¾è°±é¡¹ç›®ç»¼åˆè¯„æµ‹å®Œæˆï¼/ Knowledge Graph Project comprehensive evaluation completed!"
    log_info "æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½• / All results saved to results/ directory"
}

# é”™è¯¯å¤„ç† / Error handling
trap 'log_error "ç»¼åˆè¯„æµ‹æ‰§è¡Œå¤±è´¥ã€‚è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚"; exit 1' ERR

# æ‰§è¡Œä¸»å‡½æ•° / Execute main function
main "$@"
